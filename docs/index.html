<!DOCTYPE html>
<html>

<head>
    <title>Adversarial Knowledge Distillation for a Compact Generator</title>
    <link rel="stylesheet" type="text/css" href="./pvg.css">
    <link rel="shortcut icon" type="image/png" href="./img/cc_logo_1_crop.png">
</head>

<body>
    <script type="text/javascript" src="./header.js"></script>

    <style>
        a.myclass {
            color: #DE382D;
            text-decoration: underline
        }
    </style>

    <style>
        a.link {
            text-decoration: underline
        }
    </style>

    <header id="header">
        <div class="container">
            <div class="header">
                <h1 align="center" style="font-size: 30pt;"><b>Adversarial Knowledge Distillation <br> for a Compact
                        Generator</b></h1><br />
            </div>
        </div>
    </header>

    <h2>Abstract</h2>
    <p>
        In this paper, we propose memory-efficient Generative Adversarial Nets (GANs) in line with knowledge
        distillation. Most existing GANs have a shortcoming in terms of the number of model parameters and low
        processing speed. Here, to tackle the problem, we propose Adversarial Knowledge Distillation for Generative
        models (AKDG) for highly efficient GANs, in terms of unconditional generation. Using AKDG, model size and
        processing speed are substantively reduced. Through an adversarial training exercise with a distillation
        discriminator, a student generator successfully mimics a teacher generator in fewer model layers and fewer
        parameters and at a higher processing speed. Moreover, our AKDG is network architecture-agnostic. Comparison of
        AKDG-applied models to vanilla models suggests that it achieves closer scores to a teacher generator and more
        efficient performance than a baseline method with respect to Inception Score (IS) and Frechet Inception Distance
        (FID). In CIFAR-10 experiments, improving IS/FID 1.17pt/55.19pt and in LSUN bedroom experiments, improving FID
        71.1pt in comparison to the conventional distillation method for GANs.</p>

    <br>
    <h2>Paper</h2>
    <ul>
        <a href="https://scholar.google.com/citations?user=ZZvzcpAAAAAJ&hl=ja" class="">Hideki Tsunashima</a>,
        <a href="http://hirokatsukataoka.net/" class="">Hirokatsu Kataoka</a>,
        <a href="https://scholar.google.com/citations?user=S8DWBvMAAAAJ&hl=en" class="">Junji Yamato</a>,
        <a href="https://scholar.google.com/citations?user=9SDbpSsAAAAJ&hl=ja" class="">Qiu Chen</a>,
        <a href="https://scholar.google.co.jp/citations?user=4B-C50EAAAAJ&hl=ja" class="">Shigeo Morishima</a>. <br>
        <b>Adversarial Knowledge Distillation for a Compact Generator.</b> <br>
        In Proc of ICPR, 2020. (Code will be available until December 2020.)<br>
        <a href="" class="myclass">[paper]</a>
        &nbsp;&nbsp; <a href="" class="myclass">[bibtex]</a>
        <!--&nbsp;&nbsp; <a href="Im2Pano3D_talk.pdf"  class="myclass">[slide(.pdf)]</a> -->

    </ul>

    <br>
    <h2>Overview</h2>
    We transfer the knowledge of the teacher into the student using Adversarial Knowledge Distillation for Generative
    models (AKDG). We not only use the standard adversarial loss between the student discriminator and generator but
    also employ the adversarial loss between the teacher and student. We review the two conventional Knowledge
    Distillation
    (KD) method for GANs. First, <a href="http://proceedings.mlr.press/v97/koratana19a.html" , class="myclass">LIT</a>
    reduces residual blocks in GANs so that transfers the knowledge of the teacher into the student. However, it cannot
    be applied to GANs, which do not employ residual blocks. Second, <a href="https://arxiv.org/abs/1902.00159" ,
        class="myclass">MSE-method</a> use only Mean Squared Error (MSE) between the images generated by the teacher and
    student. However, the generated images are heavily blurred because MSE incurs the blurred results in generative
    models. As a result, the FID of the images generated by MSE-method is long. On the other hand, our AKDG not only can
    be applied to any model architectures but also achieve the state-of-the-art FID score in KD for generative models.
    <br><br><br>
    <center>
        <img src="./img/proposed_method.png" style="width: 100%;" />
    </center>

    <br><br><br>
    <h2>Example Results</h2>

    <center><img src="./img/cifar10_tab.png" style="width: 100%;" vspace="20" /></center>

    <br><br><br>
    <center><img src="./img/cifar10_result.png" style="width: 100%;" vspace="20" /></center>

    <br><br><br>
    <center><img src="./img/lsun_tab.png" style="width: 100%;" vspace="20" /></center>

    <center><img src="./img/lsun_result.png" style="width: 100%;" /></center>


    <br><br><br>
    <script type="text/javascript" src="./footer.js"></script>
</body>

</html>